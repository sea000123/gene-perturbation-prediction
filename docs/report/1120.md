### 1. scGPT 的预训练方法论 (Pre-training Methodology)

scGPT 的核心创新在于将**非序列化（Non-sequential）**的单细胞数据适配到为序列数据设计的Transformer架构中。它不完全等同于BERT（MLM）或GPT（Causal LM），而是采用了一种**生成式掩码建模（Generative Masked Modeling）**策略。

#### A. 输入表示 (Input Embeddings)
scGPT 将每一个细胞视为一个“句子”，将每一个基因视为一个“Token”。输入由三个核心Embedding相加组成：
1.  **Gene Token Embedding ($t_g$)**: 基因的唯一身份标识（类似于NLP中的Word ID）。
2.  **Expression Value Embedding ($x_{val}$)**: 这是一个关键设计。由于基因表达量是连续值且存在批次效应（Batch Effect），scGPT采用了**Value Binning（数值分箱）**技术。
    *   将非零表达值离散化为 $B$ 个区间（bins）。
    *   将离散后的整数通过一个MLP映射为Embedding。
    *   *目的*：捕捉相对表达水平，减少绝对数值带来的技术噪声。
3.  **Condition Token Embedding ($t_c$)**: 用于注入元数据，如批次信息（Batch ID）、扰动类型（Perturbation type）、模态信息等。

#### B. 核心机制：专门设计的注意力掩码 (Specially Designed Attention Mask)
单细胞数据中基因的排列没有自然顺序（Permutation Invariant）。为了进行生成式预训练，scGPT设计了特殊的Attention Mask：
*   **训练目标**：根据已知基因（Known Genes）预测未知基因（Unknown/Masked Genes）的表达值。
*   **Mask 逻辑**：
    *   **Known to Known**: 允许注意力交互（全连接）。
    *   **Unknown to Known**: 允许未知基因关注已知基因（聚合上下文信息）。
    *   **Unknown to Unknown**: **禁止**注意力交互。
    *   *Self-Attention*: 允许Query关注自身。
*   **生成式训练 (Generative Training)**：这种Mask机制允许模型在推理时通过迭代方式生成全基因组表达谱（类似于自回归，但针对的是集合而非序列）。

#### C. 损失函数 (Loss Function)
不同于NLP预测Token ID（分类问题），scGPT预测的是**基因表达值**（回归问题）。
*   **Objective**: 最小化被Mask掉的基因位置上的预测表达值与真实表达值之间的**均方误差 (MSE)**。
    $$ \mathcal{L} = \frac{1}{|\mathcal{M}|} \sum_{j \in \mathcal{M}} (\text{MLP}(h_n^{(i)}) - x_j^{(i)})^2 $$
    其中 $h_n^{(i)}$ 是Transformer输出的隐层表示，$x_j^{(i)}$ 是真实的表达值。

---

### 2. 预训练数据 (Pre-training Data)

scGPT 旨在建立一个“全人类细胞图谱”的基础模型，其数据规模和多样性是其性能的关键。

*   **数据来源**: 主要来自 **CELLxGENE** Collection (Census API)。
*   **数据规模**: 约 **3,300万 (33 Million)** 个人类单细胞。
*   **覆盖范围**:
    *   **组织**: 51个器官/组织（Organs/Tissues）。
    *   **研究**: 涵盖441个不同的研究项目（Studies）。
    *   **条件**: 主要是正常（Normal/Non-disease）状态下的细胞，用于学习通用的生物学基准。
*   **特定领域模型**: 论文中还提到了针对特定器官（如血液、大脑）和泛癌（Pan-cancer）的子模型，但核心Foundation Model是基于Whole-human数据的。

---

### 3. 模型架构 (Model Architecture)

基于 GitHub 源代码和论文描述，scGPT 的架构是标准的 **Transformer Encoder** 变体，但集成了针对连续值的适配层。

*   **Backbone**: Stacked Transformer Blocks (类似于BERT，但使用上述的生成式Mask)。
*   **主要超参数 (针对 33M 模型)**:
    *   `d_model` (Embedding Dimension): 512
    *   `n_layers`: 12
    *   `n_heads`: 8
    *   `d_ffn` (Feed Forward Hidden Size): 512
*   **加速机制**: 采用了 **FlashAttention**。这是必须的，因为单细胞的输入序列长度（基因数）通常在 1,200 到数万之间，FlashAttention 显著降低了显存占用和计算复杂度。
*   **输出层**: 一个多层感知机（MLP）头，用于将Transformer的隐层输出映射回标量表达值。

---

### 4. 迁移至 Virtual Cell Challenge (VCC) 的策略

#### 4.1 VCC 任务描述
Virtual Cell Challenge (VCC) 的核心任务是**基因扰动预测 (Gene Perturbation Prediction)**。即给定细胞的初始状态（通常为对照组/未扰动状态）和一个目标扰动基因（Target Gene），模型需要预测该基因被敲除（Knockout）后，全基因组的表达谱会如何变化。

#### 4.2 零样本推理流程 (Zero-shot Inference Pipeline)
目前采用了基于 scGPT 预训练权重的零样本推理方案，具体步骤如下：

**步骤 1: 词表对齐 (Vocabulary Alignment)**
*   加载 scGPT 的预训练词表（Vocab）。
*   将 VCC 数据集中的基因符号映射到 scGPT 的 Token ID。对于不在词表中的基因，统一标记为 `<pad>` 以避免索引越界。

**步骤 2: 构造扰动输入 (Input Construction)**
*   **采样**: 从训练集 (`train.h5ad`) 中抽取未扰动的对照组细胞（Control Cells）作为基准状态。
    *   **注入扰动**:
    *   构造一个与基因数等长的二进制向量 `pert_flags`。
    *   将**目标基因**（Target Gene）对应的位置设为 `1`，其余位置设为 `0`。
        *   *含义*：目标基因是指需模拟敲除的特定基因（如本实验中的 *TCF3, SMARCA4* 等）。
        *   *输入机制*：scGPT 采用 **Embedding 相加** 的方式处理输入。`pert_flags` 通过一个专门的 `pert_encoder` 映射为向量 $E_{pert}$，并与基因 Embedding ($E_{gene}$) 和表达量 Embedding ($E_{val}$) 相加：
            $$ E_{input} = E_{gene} + E_{val} + E_{pert} $$
        *   *与预训练的关系*：这类似于 NLP 中的 **Prompt Tuning** 或 **Conditional Generation**。
            *   此处的零样本任务是“根据扰动提示（Prompt）重构表达谱”。
            *   `pert_flags` 作为一个强提示符，激活模型内部学到的该基因相关调控通路，引导模型生成扰动后的状态，而非仅仅复制对照组的输入。
    *   模型输入为 `[Expression Values, Perturbation Flags]` 的堆叠张量。

**步骤 3: 模型推理 (Model Inference)**
*   调用 scGPT 的 `pred_perturb` 接口。
*   模型内部利用 Transformer 的注意力机制，根据输入的扰动 Flag，结合预训练学到的基因调控网络知识，调整全基因组的预测表达值。
*   *注*：此过程不涉及梯度更新（No Gradient Update），纯粹利用模型已有的知识进行推断。

**步骤 4: 伪批量评估 (Pseudo-bulk Evaluation)**
*   **维度变化**：
    *   **Before Mean (单细胞层级)**：输入是一个矩阵 $(N_{cells}, N_{genes})$，其中 $N_{cells}$ 是预测或真实的细胞数量（例如 100 个细胞），$N_{genes}$ 是基因数量。每一行代表一个单独细胞的表达谱。
    *   **After Mean (伪批量层级)**：对细胞维度（axis=0）求均值，输出是一个向量 $(1, N_{genes})$。
*   **目的**：单细胞数据存在高噪声和稀疏性（Dropout）。通过取均值（Pseudo-bulk），我们可以消除个体细胞的随机噪声，聚焦于该扰动条件下细胞群体的**平均响应趋势**，从而更准确地评估模型对扰动效应的捕捉能力。
*   计算两者之间的 Pearson 相关系数和 MSE 进行评估。

### 5. 评估指标 (Evaluation Metrics)

针对零样本扰动预测，主要使用以下指标：

1.  **皮尔逊相关系数 (Pearson Correlation)**
    *   **计算对象**：针对每一个扰动（Target Gene），计算**预测的基因表达均值向量**（Predicted Mean Vector）与**真实的基因表达均值向量**（Ground Truth Mean Vector）之间的相关性。这两个向量的长度均为全基因组的基因数量（$N_{genes}$）。
    *   **含义**：衡量预测表达谱与真实值在**变化趋势**上的相似度。
    TODO: Predicted - Ground Truth and Actual - Ground Truth 做 Pearson.
    *   **理由**：生物学分析更关注基因间的相对调控模式（Pattern）。高相关性意味着模型成功复现了细胞状态的整体“形状”，即使绝对数值有偏差。

2.  **均方误差 (MSE)**
    *   **含义**：衡量预测值与真实值之间**数值差异**的大小。
    *   **理由**：用于约束预测结果的绝对量级（Magnitude）。它惩罚过大的数值偏差，确保预测出的表达量在生物学上是合理的。

### 总结
将 scGPT 迁移到 VCC 的核心在于将 VCC 的“预测扰动后状态”问题转化为 scGPT 的 **"Conditioned Generation"** 问题。利用 scGPT 在 33M 细胞上学到的基因共表达流形（Gene Co-expression Manifold），通过微调让模型学会将“扰动 Token”映射为流形上的位移向量。