# scGPT Finetuning Configuration for VCC Perturbation Prediction
# Based on scGPT Tutorial_Perturbation.ipynb
# This config is used for BOTH training and evaluation

# ========== Paths ==========
paths:
  # Pretrained model directory (used for training)
  model_dir: "model/scGPT"
  
  # Finetuned model directory (output for training, input for eval)
  finetuned_model_dir: "model/scGPT_finetuned"
  
  # Evaluation data paths
  data_dir: "data/processed"
  test_file: "test.h5ad"
  train_file: "train.h5ad"
  
  # Evaluation results output
  output_dir: "results"
  
  # GEARS data directory (for training, created by scripts/convert_to_gears.py)
  gears_data_dir: "data/processed/gears"
  dataset_name: "vcc"

# ========== Model ==========
# Common model settings used by both training and evaluation
model:
  pad_token: "<pad>"
  pad_value: 0
  max_seq_len: 1536
  include_zero_gene: "batch-wise"  # "batch-wise" uses less memory than "all"
  
  # Architecture (overridden by pretrained model config args.json during training)
  embsize: 512
  d_hid: 512
  nlayers: 12
  nhead: 8
  n_layers_cls: 3
  dropout: 0.0
  use_fast_transformer: false  # Disabled: flash-attn not available

# ========== Inference / Evaluation ==========
inference:
  batch_size: 16
  eval_batch_size: 16  # Batch size for evaluation (processes all cells batch by batch)
  seed: 42
  control_target_gene: "non-targeting"

# ========== Data Processing (Training) ==========
data:
  special_tokens: ["<pad>", "<cls>", "<eoc>"]
  pert_pad_id: 0

# ========== Training Objectives ==========
training:
  # Primary objective (always on for perturbation)
  MLM: true   # Masked Language Modeling
  
  # Optional objectives (disabled for perturbation finetuning)
  CLS: false  # Cell type classification
  CCE: false  # Contrastive cell embedding
  MVC: false  # Masked value prediction for cell embedding
  ECS: false  # Elastic cell similarity
  
  # Mixed precision training
  amp: true
  
  # ========== Freeze Encoder (P0 Fix) ==========
  # CRITICAL: Set to true to prevent catastrophic forgetting
  # With only ~100 training perturbations, updating the encoder destroys
  # pretrained gene-gene relationships learned from 33M cells
  freeze_encoder: true
  freeze_prefixes:
    - "encoder"
    - "value_encoder"
    - "transformer_encoder"
  
  # ========== Weighted Loss (P1 Fix) ==========
  # Focus training on high-change genes (aligns with Top2000-MAE eval metric)
  weighted_loss: true
  weight_factor: 10.0  # Upweight high-change genes by this factor

# ========== Optimizer ==========
optimizer:
  lr: 1.0e-4
  batch_size: 64
  eval_batch_size: 64
  epochs: 15
  schedule_interval: 1
  schedule_gamma: 0.9
  early_stop: 10  # patience for early stopping
  grad_clip: 1.0

# ========== Loss Function ==========
loss:
  type: "SmoothL1Loss"  # Options: "SmoothL1Loss", "MSELoss", "L1Loss"
  beta: 0.5             # For SmoothL1Loss only (transition from L1 to L2)
  reduction: "mean"     # Options: "mean", "sum", "none"

# ========== Partial Weight Loading ==========
# Only load these prefixes from pretrained model
# The rest (e.g., decoder) will be randomly initialized
load_param_prefixes:
  - "encoder"
  - "value_encoder"
  - "transformer_encoder"

# ========== Data Split ==========
# Train/Val split on 120 genes (excluding 30 test genes)
split:
  test_genes_file: "data/raw/test_set.csv"  # 30 test genes
  train_ratio: 0.833  # 100/120 genes for training
  val_ratio: 0.167    # 20/120 genes for validation
  seed: 42

# ========== Metrics ==========
metrics:
  mae_top_k: 2000  # Top K genes by |log2FC| for MAE calculation

# ========== Early Stopping ==========
early_stopping:
  metric: "overall_score"  # Higher is better (0-100 scale per eval_metrics.md)

# ========== Logging ==========
logging:
  log_interval: 100
  save_interval: 1  # Save every epoch

# ========== Hardware ==========
hardware:
  # Detected automatically, but can override
  device: "cuda"  # "cuda" or "cpu"
